"""
eval pretrained model with enhanced visualization and deploy-oriented JSON export.

æŒ‡ä»¤ï¼š
python training/test.py --detector_path training/config/detector/xception.yaml --test_dataset UADFV --weights_path training/weights/xception_best.pth --output_dir training/outputs/xception_uadfv --save_curves --save_tsne --save_timelines --save_keyframes --save_json --task_id bench-uadfv-xception

ucf:
python training/test.py --detector_name ucf --test_dataset UADFV --output_dir training/outputs/ucf_uadfv --save_curves --save_tsne --save_timelines --save_keyframes --save_json --task_id bench-uadfv-ucf

effort:
python training/test.py --detector_name effort --test_dataset UADFV --output_dir training/outputs/effort_uadfv --save_curves --save_tsne --save_timelines --save_keyframes --save_json --task_id bench-uadfv-effort


"""

import os
import json
import numpy as np
from os.path import join, dirname, abspath
import cv2
import random
import time
import yaml
from tqdm import tqdm
import argparse
from collections import defaultdict

import torch
import torch.nn.functional as F
import torch.utils.data

from dataset.abstract_dataset import DeepfakeAbstractBaseDataset
from detectors import DETECTOR
from metrics.utils import get_test_metrics

# æ–°å¢ï¼šå¯è§†åŒ–ä¸å¯¼å‡ºå·¥å…·
from utils.vis import (
    plot_roc_pr_curves,
    plot_confusion,
    plot_tsne_features,
    plot_video_timelines,
    save_keyframe_thumbnails, plot_video_score_distribution,
)
from utils.json_export import (
    build_deploy_json_per_dataset,
    build_detectionresponse_for_backend,
    post_result_to_backend,
)

DETECTOR_CONFIG_MAP = {
    'xception': 'training/config/detector/xception.yaml',
    'ucf':      'training/config/detector/ucf.yaml',
    'effort':   'training/config/detector/effort.yaml',  # å¦‚æœåé¢è¦åŠ  Effort
}

DETECTOR_WEIGHTS_MAP = {
    'xception': 'training/weights/xception_best.pth',
    'ucf':      'training/weights/ucf_best.pth',
    'effort':   'training/weights/effort_clip_L14.pth',
}


parser = argparse.ArgumentParser(description='Enhanced Test with Visualization and JSON Export')

parser.add_argument('--detector_name',type=str,default=None,choices=['xception', 'ucf', 'effort'],help='name of detector (e.g., xception / ucf / effort); '
'if set, detector_path and weights_path can be inferred'
)

# 2) å°† detector_path / weights_path æ”¹ä¸ºå¯é€‰ï¼ˆå’Œ detector_name äº’æ–¥ä½¿ç”¨ï¼‰
parser.add_argument(
    '--detector_path',
    type=str,
    default=None,
    help='path to detector YAML file (override detector_name mapping)'
)

parser.add_argument(
    "--test_dataset",
    nargs="+",
    required=True,
    help='one or more dataset names to test'
)

parser.add_argument('--weights_path',type=str,default=None,help='path to model weights (override detector_name mapping)')
# æ–°å¢å¯¼å‡ºä¸å¯è§†åŒ–å‚æ•°
parser.add_argument('--output_dir', type=str, default='./training/outputs', help='dir to save figures and json')
parser.add_argument('--save_json', action='store_true', default=False, help='export detailed json per dataset')
parser.add_argument('--save_curves', action='store_true', default=False, help='save ROC/PR curves and confusion matrix')
parser.add_argument('--save_tsne', action='store_true', default=False, help='save t-SNE feature visualization')
parser.add_argument('--save_timelines', action='store_true', default=False, help='save per-video timeline heatmaps')
parser.add_argument('--save_keyframes', action='store_true', default=False,
                    help='export top-K suspicious frame thumbnails')
parser.add_argument('--topk', type=int, default=8, help='top-K suspicious frames per video')
parser.add_argument('--post_backend', action='store_true', default=False, help='post DetectionResponse JSON to backend')
parser.add_argument('--backend_url', type=str, default='http://backend:8080/api/v1/detections/results',
                    help='backend endpoint')
parser.add_argument('--task_id', type=str, default='', help='task id for backend DetectionResponse packaging')
parser.add_argument('--model_version', type=str, default='DeepfakeBench-v2', help='model version string for packaging')



args = parser.parse_args()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def init_seed(config):
    if config.get('manualSeed', None) is None:
        config['manualSeed'] = random.randint(1, 10000)
    random.seed(config['manualSeed'])
    torch.manual_seed(config['manualSeed'])
    if config.get('cuda', False):
        torch.cuda.manual_seed_all(config['manualSeed'])


def prepare_testing_data(config):
    def get_test_data_loader(config, test_name):
        cfg = config.copy()
        cfg['test_dataset'] = test_name
        test_set = DeepfakeAbstractBaseDataset(config=cfg, mode='test')
        test_loader = torch.utils.data.DataLoader(
            dataset=test_set,
            batch_size=cfg['test_batchSize'],
            shuffle=False,
            num_workers=int(cfg['workers']),
            collate_fn=test_set.collate_fn,
            drop_last=False
        )
        return test_loader, test_set  # è¿”å› dataset ç”¨äºè·å–è·¯å¾„

    loaders = {}
    datasets = {}
    for one_test_name in config['test_dataset']:
        loader, dataset = get_test_data_loader(config, one_test_name)
        loaders[one_test_name] = loader
        datasets[one_test_name] = dataset
    return loaders, datasets


@torch.no_grad()
def inference(model, data_dict):
    return model(data_dict, inference=True)


def derive_video_id(img_path: str) -> str:
    """
    ä»å›¾åƒè·¯å¾„æå–è§†é¢‘ ID
    ä¾‹å¦‚: .../UADFV/frames/fake/video_001/frame_0012.png -> fake/video_001
    """
    if not img_path:
        return "unknown"

    parts = img_path.replace('\\', '/').split('/')

    # å°è¯•æ‰¾åˆ° frames ç›®å½•åçš„è·¯å¾„
    try:
        frames_idx = parts.index('frames')
        # å– frames åé¢çš„ä¸¤çº§ä½œä¸ºè§†é¢‘ IDï¼ˆç±»åˆ«/è§†é¢‘åï¼‰
        if len(parts) > frames_idx + 2:
            return '/'.join(parts[frames_idx + 1:frames_idx + 3])
    except (ValueError, IndexError):
        pass

    # å¤‡é€‰æ–¹æ¡ˆï¼šå–å€’æ•°ç¬¬äºŒçº§ç›®å½•
    if len(parts) >= 2:
        return parts[-2]

    return "unknown"


def test_one_dataset(model, data_loader, dataset):
    prob_list, label_list, feat_list, img_list, vid_index = [], [], [], [], []
    sample_idx = 0

    for batch_idx, data_dict in tqdm(enumerate(data_loader), total=len(data_loader)):
        data, label, mask, landmark = data_dict['image'], data_dict['label'], data_dict['mask'], data_dict['landmark']
        # äºŒå€¼åŒ–æ ‡ç­¾ï¼š>=1 è§†ä¸º fake
        label_bin = torch.where(label != 0, 1, 0)

        data_dict['image'], data_dict['label'] = data.to(device), label_bin.to(device)
        if mask is not None:
            data_dict['mask'] = mask.to(device)
        if landmark is not None:
            data_dict['landmark'] = landmark.to(device)

        # ===== ç»Ÿä¸€æ¨ç†æ¥å£ =====
        pred = inference(model, data_dict)

        # 1) å–å¾—æ¦‚ç‡ï¼š
        #    - è‹¥ detector å·²ç»ç»™äº† 'prob'ï¼ˆå¦‚ Xception/F3Net ç­‰ï¼‰ï¼Œç›´æ¥ç”¨
        #    - è‹¥æ²¡æœ‰ï¼ˆå¦‚ UCFï¼‰ï¼Œä» 'cls' é‡Œ softmax è®¡ç®—
        if 'prob' in pred:
            prob_tensor = pred['prob']
        else:
            logits = pred['cls']          # shape: [B, 2]
            prob_tensor = torch.softmax(logits, dim=1)[:, 1]

        prob = prob_tensor.detach().cpu().numpy()

        # 2) å–å¾—ç‰¹å¾ï¼š
        feat_tensor = pred['feat']       # æ‰€æœ‰ detector éƒ½æœ‰ 'feat'
        feat = feat_tensor.detach().cpu().numpy()

        prob_list.extend(list(prob))
        label_list.extend(list(label_bin.cpu().numpy()))
        feat_list.extend(list(feat))

        # === è·¯å¾„ä¸ video_idï¼ˆä½ ä¹‹å‰çš„é€»è¾‘ï¼Œè¿™é‡Œä¿æŒä¸å˜ï¼‰ ===
        batch_size = len(prob)
        for i in range(batch_size):
            if sample_idx < len(dataset.image_list):
                img_path = dataset.image_list[sample_idx]
                img_list.append(img_path)
                vid_index.append(derive_video_id(img_path))
            else:
                img_list.append(f"unknown_{sample_idx}")
                vid_index.append("unknown")
            sample_idx += 1

    return (
        np.array(prob_list).reshape(-1),
        np.array(label_list).reshape(-1),
        np.array(feat_list),
        img_list,
        vid_index
    )

def test_epoch(model, test_data_loaders, test_datasets, out_dir, options):
    """æµ‹è¯•ä¸»å‡½æ•°"""
    model.eval()
    metrics_all, per_dataset_details = {}, {}
    os.makedirs(out_dir, exist_ok=True)

    for ds_name in test_data_loaders.keys():
        loader = test_data_loaders[ds_name]
        dataset = test_datasets[ds_name]

        print(f"\n{'=' * 60}")
        print(f"æµ‹è¯•æ•°æ®é›†: {ds_name}")
        print(f"{'=' * 60}")

        predictions, labels, feats, img_paths, video_ids = test_one_dataset(model, loader, dataset)

        ds_dir = join(out_dir, ds_name)
        os.makedirs(ds_dir, exist_ok=True)

        # æŒ‡æ ‡è®¡ç®—
        metrics = get_test_metrics(y_pred=predictions, y_true=labels, img_names=img_paths)
        metrics_all[ds_name] = metrics

        print(f"å¸§çº§ AUC: {metrics.get('frame_auc', 0):.4f}")
        print(f"è§†é¢‘çº§ AUC: {metrics.get('video_auc', 0):.4f}")

        # å¯è§†åŒ–æ›²çº¿
        cm_path = None
        if options['save_curves']:
            print("ç”Ÿæˆ ROC/PR æ›²çº¿å’Œæ··æ·†çŸ©é˜µ...")
            plot_roc_pr_curves(labels, predictions, save_dir=ds_dir, prefix='curves')
            cm_path = plot_confusion(labels, predictions, save_dir=ds_dir, prefix='confusion')

        # t-SNE
        tsne_path = None
        if options['save_tsne'] and feats.shape[0] > 10:
            print("ç”Ÿæˆ t-SNE ç‰¹å¾å¯è§†åŒ–...")
            tsne_path = plot_tsne_features(feats, labels, save_dir=ds_dir, prefix='tsne')

        # æŒ‰è§†é¢‘èšåˆ
        video_map = defaultdict(lambda: {'scores': [], 'frames': [], 'labels': []})
        for img_path, pred, label in zip(img_paths, predictions.tolist(), labels.tolist()):
            vid = derive_video_id(img_path)
            video_map[vid]['scores'].append(float(pred))
            video_map[vid]['frames'].append(img_path)
            video_map[vid]['labels'].append(int(label))

        print(f"æ£€æµ‹åˆ° {len(video_map)} ä¸ªè§†é¢‘")

        # ğŸ†• ä¼ä¸šçº§å¯è§†åŒ–ï¼šè§†é¢‘åˆ†æ•°åˆ†å¸ƒå›¾
        distribution_plot = None
        if options.get('save_distribution', True):  # é»˜è®¤å¼€å¯
            print("ç”Ÿæˆè§†é¢‘åˆ†æ•°åˆ†å¸ƒå›¾...")
            distribution_plot = plot_video_score_distribution(video_map, save_dir=ds_dir, prefix='distribution')

        # ğŸ”§ ä¼˜åŒ–ï¼šåªä¸º Top-K è§†é¢‘ç”Ÿæˆæ—¶é—´è½´
        timeline_paths = {}
        if options['save_timelines'] and len(video_map) > 0:
            print(f"ç”Ÿæˆ Top-{options.get('max_timeline_videos', 10)} å¯ç–‘è§†é¢‘çš„æ—¶é—´è½´...")
            timeline_paths = plot_video_timelines(
                video_map,
                save_dir=ds_dir,
                prefix='timeline',
                max_videos=options.get('max_timeline_videos', 10)  # é»˜è®¤ 10 ä¸ª
            )

        # ğŸ”§ ä¼˜åŒ–ï¼šåªä¸º Top-K è§†é¢‘æå–å…³é”®å¸§
        keyframe_paths = {}
        if options['save_keyframes'] and len(video_map) > 0:
            print(f"æå– Top-{options.get('max_keyframe_videos', 10)} å¯ç–‘è§†é¢‘çš„å…³é”®å¸§...")
            keyframe_paths = save_keyframe_thumbnails(
                video_map,
                topk=options['topk'],
                save_dir=ds_dir,
                prefix='keyframes',
                max_videos=options.get('max_keyframe_videos', 10)  # é»˜è®¤ 10 ä¸ª
            )

        # ç»„è£…æ•°æ®é›†æ˜ç»†
        per_dataset_details[ds_name] = {
            'predictions': predictions.tolist(),
            'labels': labels.tolist(),
            'image_paths': img_paths,
            'video_map': video_map,
            'curves_dir': ds_dir if options['save_curves'] else None,
            'confusion_path': cm_path,
            'tsne_path': tsne_path,
            'distribution_plot': distribution_plot,  # ğŸ†• æ–°å¢
            'timeline_paths': timeline_paths,
            'keyframe_paths': keyframe_paths,
            'metrics': metrics
        }

    return metrics_all, per_dataset_details

def main():
    t0 = time.time()

    detector_path = args.detector_path
    weights_path = args.weights_path

    if args.detector_name is not None:
        # ä»æ˜ å°„è¡¨è·å–é»˜è®¤è·¯å¾„
        if detector_path is None:
            if args.detector_name not in DETECTOR_CONFIG_MAP:
                raise ValueError(f"æœªçŸ¥çš„ detector_name: {args.detector_name}")
            detector_path = DETECTOR_CONFIG_MAP[args.detector_name]
        if weights_path is None:
            if args.detector_name not in DETECTOR_WEIGHTS_MAP:
                raise ValueError(f"æ²¡æœ‰ä¸º {args.detector_name} é…ç½®é»˜è®¤æƒé‡è·¯å¾„")
            weights_path = DETECTOR_WEIGHTS_MAP[args.detector_name]

    # ä»ç„¶å…è®¸è€ç”¨æ³•ï¼šå¿…é¡»ä¿è¯æœ€ç»ˆæœ‰è·¯å¾„
    if detector_path is None or weights_path is None:
        raise ValueError(
            "å¿…é¡»è‡³å°‘æ»¡è¶³ä»¥ä¸‹ä¸¤ç§æ–¹å¼ä¹‹ä¸€ï¼š\n"
            "1) ä»…æŒ‡å®š --detector_nameï¼ˆè‡ªåŠ¨æ˜ å°„ config å’Œ weightsï¼‰\n"
            "2) æ‰‹åŠ¨æŒ‡å®š --detector_path å’Œ --weights_path"
        )

    print(f"[INFO] ä½¿ç”¨ detector: {args.detector_name or 'from_yaml'}")
    print(f"[INFO] detector_config: {detector_path}")
    print(f"[INFO] weights_path:    {weights_path}")

    # =========================================================
    # 2) åŠ è½½é…ç½® / æ¨¡å‹ï¼ˆä½¿ç”¨ä¸Šé¢è§£æå‡ºçš„è·¯å¾„ï¼‰
    # =========================================================
    with open(detector_path, 'r') as f:
        config = yaml.safe_load(f)
    with open('./training/config/test_config.yaml', 'r') as f:
        cfg2 = yaml.safe_load(f)
    config.update(cfg2)
    if 'label_dict' in config:
        cfg2['label_dict'] = config['label_dict']

    # è¦†ç›–æµ‹è¯•é›†ä¸æƒé‡
    config['test_dataset'] = args.test_dataset
    config['weights_path'] = weights_path

    init_seed(config)

    test_loaders, test_datasets = prepare_testing_data(config)

    # åŠ è½½æ¨¡å‹
    model_class = DETECTOR[config['model_name']]
    model = model_class(config).to(device)

    # åŠ è½½æƒé‡
    ckpt = torch.load(weights_path, map_location=device)

    # ========== ğŸ”§ å¤„ç†æƒé‡ key çš„å„ç§æƒ…å†µ ==========
    print(f"åŸå§‹æƒé‡ keys ç¤ºä¾‹: {list(ckpt.keys())[:3]}")

    # 1. å»æ‰ DataParallel çš„ module. å‰ç¼€
    if list(ckpt.keys())[0].startswith('module.'):
        print("âš ï¸  æ£€æµ‹åˆ° DataParallel æƒé‡ï¼ˆæœ‰ module. å‰ç¼€ï¼‰ï¼Œæ­£åœ¨è½¬æ¢...")
        new_ckpt = {}
        for k, v in ckpt.items():
            new_key = k.replace('module.', '', 1)
            new_ckpt[new_key] = v
        ckpt = new_ckpt
        print("âœ… å»é™¤ module. å‰ç¼€å®Œæˆ")

    # 2. å¤„ç† Effort çš„ vision_model è·¯å¾„å·®å¼‚
    # æƒé‡: backbone.embeddings.xxx
    # æ¨¡å‹: backbone.vision_model.embeddings.xxx
    if config['model_name'] == 'effort':
        first_key = list(ckpt.keys())[0]
        if first_key.startswith('backbone.') and 'vision_model' not in first_key:
            print("âš ï¸  æ£€æµ‹åˆ° Effort æƒé‡è·¯å¾„ä¸åŒ¹é…ï¼Œæ­£åœ¨è½¬æ¢...")
            new_ckpt = {}
            for k, v in ckpt.items():
                if k.startswith('backbone.'):
                    # backbone.embeddings.xxx -> backbone.vision_model.embeddings.xxx
                    new_key = k.replace('backbone.', 'backbone.vision_model.', 1)
                    new_ckpt[new_key] = v
                else:
                    # head.weight ç­‰ä¿æŒä¸å˜
                    new_ckpt[k] = v
            ckpt = new_ckpt
            print("âœ… Effort æƒé‡è·¯å¾„è½¬æ¢å®Œæˆ")

    print(f"è½¬æ¢åæƒé‡ keys ç¤ºä¾‹: {list(ckpt.keys())[:3]}")
    # ========== ä¿®å¤ç»“æŸ ==========

    model.load_state_dict(ckpt, strict=False)
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {config['model_name']}")

    # è¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)

    # å¯è§†åŒ–é€‰é¡¹
    options = {
        'save_curves': args.save_curves,
        'save_tsne': args.save_tsne,
        'save_timelines': args.save_timelines,
        'save_keyframes': args.save_keyframes,
        'save_distribution': True,  # ğŸ†• é»˜è®¤å¼€å¯åˆ†å¸ƒå›¾
        'topk': args.topk,
        'max_timeline_videos': 10,  # ğŸ”§ åªä¸º Top-10 è§†é¢‘ç”Ÿæˆæ—¶é—´è½´
        'max_keyframe_videos': 10   # ğŸ”§ åªä¸º Top-10 è§†é¢‘æå–å…³é”®å¸§
    }

    # æ‰§è¡Œæµ‹è¯•
    metrics_all, details = test_epoch(model, test_loaders, test_datasets, args.output_dir, options)

    print('\n' + '=' * 60)
    print('ğŸ‰ æµ‹è¯•å®Œæˆï¼')
    print('=' * 60)

    # æ‰“å°æŒ‡æ ‡æ‘˜è¦
    for ds_name, metrics in metrics_all.items():
        print(f"\n{ds_name}:")
        print(f"  å¸§çº§ AUC: {metrics.get('frame_auc', 0):.4f}")
        print(f"  è§†é¢‘çº§ AUC: {metrics.get('video_auc', 0):.4f}")

    # å¯¼å‡º JSON
    if args.save_json:
        print("\nç”Ÿæˆ JSON å¯¼å‡ºæ–‡ä»¶...")
        deploy_json_path = build_deploy_json_per_dataset(
            output_dir=args.output_dir,
            all_details=details,
            all_metrics=metrics_all,
            task_id=args.task_id or 'benchmark-task',
            model_version=args.model_version
        )
        print(f'âœ… JSON å·²ä¿å­˜: {deploy_json_path}')

        # å¯é€‰ï¼šå›ä¼ åç«¯
        if args.post_backend:
            print("\nå‘é€ç»“æœåˆ°åç«¯...")
            detection_json = build_detectionresponse_for_backend(
                all_details=details,
                task_id=args.task_id or 'benchmark-task',
                model_version=args.model_version
            )
            ok, resp_text = post_result_to_backend(args.backend_url, detection_json)
            if ok:
                print(f'âœ… åç«¯æ¥æ”¶æˆåŠŸ')
            else:
                print(f'âŒ åç«¯æ¥æ”¶å¤±è´¥: {resp_text}')

    print(f'\næ€»è€—æ—¶: {time.time() - t0:.2f}ç§’')


if __name__ == '__main__':
    main()
